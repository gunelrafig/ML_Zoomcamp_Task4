{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8a34144",
   "metadata": {},
   "source": [
    "\n",
    "# Lead Scoring Homework â€” Bank Marketing Dataset\n",
    "\n",
    "This notebook solves the lead scoring classification task using the Bank Marketing dataset.  \n",
    "Steps included:\n",
    "1. Data Preparation (missing values handling and splitting)\n",
    "2. ROC AUC feature importance\n",
    "3. Logistic Regression model training\n",
    "4. Precision, Recall, and F1-score threshold analysis\n",
    "5. 5-Fold Cross-Validation evaluation\n",
    "6. Hyperparameter tuning for Logistic Regression\n",
    "\n",
    "All comments in code are in **English**, as requested.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da6bda0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load dataset\n",
    "DATA_PATH = 'bank_marketing_leads.csv'  # change if needed\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "print(\"Dataset shape:\", df.shape)\n",
    "display(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d4b1c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Data preparation: handle missing values\n",
    "# For categorical features -> fill with 'NA'\n",
    "# For numerical features -> fill with 0.0\n",
    "\n",
    "df_prep = df.copy()\n",
    "\n",
    "cat_cols = df_prep.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "num_cols = df_prep.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "print(\"Categorical columns:\", cat_cols)\n",
    "print(\"Numerical columns:\", num_cols)\n",
    "\n",
    "df_prep[cat_cols] = df_prep[cat_cols].fillna('NA')\n",
    "df_prep[num_cols] = df_prep[num_cols].fillna(0.0)\n",
    "\n",
    "print(\"Missing values after filling:\")\n",
    "print(df_prep.isnull().sum().loc[lambda s: s > 0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d9703c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Split dataset into Train (60%), Validation (20%), Test (20%)\n",
    "TARGET = 'converted'  # update if target column has another name\n",
    "\n",
    "if df_prep[TARGET].dtype == 'object':\n",
    "    df_prep[TARGET] = df_prep[TARGET].map({'yes': 1, 'no': 0})\n",
    "\n",
    "df_train_full, df_test = train_test_split(df_prep, test_size=0.2, random_state=1, stratify=df_prep[TARGET])\n",
    "df_train, df_val = train_test_split(df_train_full, test_size=0.25, random_state=1, stratify=df_train_full[TARGET])\n",
    "\n",
    "print(f\"Train: {df_train.shape}, Validation: {df_val.shape}, Test: {df_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f02122",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "features_num = ['lead_score', 'number_of_courses_viewed', 'interaction_count', 'annual_income']\n",
    "\n",
    "auc_scores = {}\n",
    "for f in features_num:\n",
    "    auc = roc_auc_score(df_train[TARGET], df_train[f])\n",
    "    if auc < 0.5:\n",
    "        auc = roc_auc_score(df_train[TARGET], -df_train[f])\n",
    "    auc_scores[f] = auc\n",
    "\n",
    "print(\"AUC scores:\")\n",
    "print(auc_scores)\n",
    "\n",
    "best_feature = max(auc_scores, key=auc_scores.get)\n",
    "print(\"\\nBest numerical variable:\", best_feature)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f99ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Prepare train and validation sets\n",
    "features = [c for c in df_train.columns if c != TARGET]\n",
    "\n",
    "dv = DictVectorizer(sparse=False)\n",
    "train_dicts = df_train[features].to_dict(orient='records')\n",
    "val_dicts = df_val[features].to_dict(orient='records')\n",
    "\n",
    "X_train = dv.fit_transform(train_dicts)\n",
    "X_val = dv.transform(val_dicts)\n",
    "y_train = df_train[TARGET].values\n",
    "y_val = df_val[TARGET].values\n",
    "\n",
    "# Train Logistic Regression\n",
    "model = LogisticRegression(solver='liblinear', C=1.0, max_iter=1000)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "y_val_pred = model.predict_proba(X_val)[:, 1]\n",
    "val_auc = roc_auc_score(y_val, y_val_pred)\n",
    "print(\"Validation AUC:\", round(val_auc, 3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a55edff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "thresholds = np.arange(0.0, 1.01, 0.01)\n",
    "precisions = []\n",
    "recalls = []\n",
    "\n",
    "for t in thresholds:\n",
    "    y_pred = (y_val_pred >= t).astype(int)\n",
    "    precisions.append(precision_score(y_val, y_pred, zero_division=0))\n",
    "    recalls.append(recall_score(y_val, y_pred))\n",
    "\n",
    "plt.figure(figsize=(7,5))\n",
    "plt.plot(thresholds, precisions, label='Precision')\n",
    "plt.plot(thresholds, recalls, label='Recall')\n",
    "plt.legend()\n",
    "plt.xlabel('Threshold')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Precision and Recall vs Threshold')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "diff = np.abs(np.array(precisions) - np.array(recalls))\n",
    "threshold_intersection = thresholds[np.argmin(diff)]\n",
    "print(\"Precision = Recall at threshold:\", round(threshold_intersection, 3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd0da45a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "f1_scores = []\n",
    "for p, r in zip(precisions, recalls):\n",
    "    if (p + r) == 0:\n",
    "        f1_scores.append(0)\n",
    "    else:\n",
    "        f1_scores.append(2 * p * r / (p + r))\n",
    "\n",
    "best_t = thresholds[np.argmax(f1_scores)]\n",
    "plt.figure(figsize=(7,5))\n",
    "plt.plot(thresholds, f1_scores)\n",
    "plt.title('F1 Score vs Threshold')\n",
    "plt.xlabel('Threshold')\n",
    "plt.ylabel('F1')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "print(\"Best F1 threshold:\", round(best_t, 3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f673411f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=1)\n",
    "scores = []\n",
    "\n",
    "for train_idx, val_idx in kf.split(df_train_full):\n",
    "    df_t = df_train_full.iloc[train_idx]\n",
    "    df_v = df_train_full.iloc[val_idx]\n",
    "\n",
    "    X_t = dv.fit_transform(df_t[features].to_dict(orient='records'))\n",
    "    X_v = dv.transform(df_v[features].to_dict(orient='records'))\n",
    "    y_t = df_t[TARGET].values\n",
    "    y_v = df_v[TARGET].values\n",
    "\n",
    "    model = LogisticRegression(solver='liblinear', C=1.0, max_iter=1000)\n",
    "    model.fit(X_t, y_t)\n",
    "    y_v_pred = model.predict_proba(X_v)[:, 1]\n",
    "    auc = roc_auc_score(y_v, y_v_pred)\n",
    "    scores.append(auc)\n",
    "\n",
    "print(\"AUC scores for 5 folds:\", np.round(scores, 3))\n",
    "print(\"Mean AUC:\", round(np.mean(scores), 3))\n",
    "print(\"Std deviation:\", round(np.std(scores), 3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "732cad60",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "C_values = [0.000001, 0.001, 1]\n",
    "cv_results = []\n",
    "\n",
    "for c in C_values:\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=1)\n",
    "    fold_scores = []\n",
    "\n",
    "    for train_idx, val_idx in kf.split(df_train_full):\n",
    "        df_t = df_train_full.iloc[train_idx]\n",
    "        df_v = df_train_full.iloc[val_idx]\n",
    "\n",
    "        X_t = dv.fit_transform(df_t[features].to_dict(orient='records'))\n",
    "        X_v = dv.transform(df_v[features].to_dict(orient='records'))\n",
    "        y_t = df_t[TARGET].values\n",
    "        y_v = df_v[TARGET].values\n",
    "\n",
    "        model = LogisticRegression(solver='liblinear', C=c, max_iter=1000)\n",
    "        model.fit(X_t, y_t)\n",
    "        y_v_pred = model.predict_proba(X_v)[:, 1]\n",
    "        auc = roc_auc_score(y_v, y_v_pred)\n",
    "        fold_scores.append(auc)\n",
    "\n",
    "    mean_score = np.mean(fold_scores)\n",
    "    std_score = np.std(fold_scores)\n",
    "    cv_results.append((c, mean_score, std_score))\n",
    "\n",
    "cv_df = pd.DataFrame(cv_results, columns=['C', 'mean_auc', 'std_auc'])\n",
    "display(cv_df)\n",
    "\n",
    "best_row = cv_df.loc[cv_df['mean_auc'].idxmax()]\n",
    "print(\"\\nBest C value:\", best_row['C'])\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
